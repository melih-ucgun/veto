package fleet

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/melih-ucgun/veto/internal/config"
	"github.com/melih-ucgun/veto/internal/core"
	"github.com/melih-ucgun/veto/internal/inventory"
	"github.com/melih-ucgun/veto/internal/system"
	"github.com/melih-ucgun/veto/internal/transport"
	"github.com/pterm/pterm"
)

// FleetManager orchestrates operations across multiple hosts.
type FleetManager struct {
	Hosts    []inventory.Host
	DiffMode bool // For Plan mode
	DryRun   bool
	Prune    bool
}

// NewFleetManager creates a new FleetManager.
func NewFleetManager(hosts []inventory.Host, dryRun bool, prune bool) *FleetManager {
	return &FleetManager{
		Hosts:  hosts,
		DryRun: dryRun,
		Prune:  prune,
	}
}

// ApplyConfig executes the given plan on all hosts in parallel.
// It accepts layers of ConfigItems (sorted by dependency).
func (f *FleetManager) ApplyConfig(layers [][]core.ConfigItem, concurrency int, createFn core.ResourceCreator) error {
	var wg sync.WaitGroup
	errChan := make(chan error, len(f.Hosts))
	sem := make(chan struct{}, concurrency) // Semaphore for concurrency control

	pterm.DefaultSection.Printf("Fleet Deployment: %d hosts (Concurrency: %d)", len(f.Hosts), concurrency)

	for _, host := range f.Hosts {
		wg.Add(1)
		go func(h inventory.Host) {
			defer wg.Done()
			sem <- struct{}{}        // Acquire
			defer func() { <-sem }() // Release

			prefix := pterm.NewStyle(pterm.FgCyan, pterm.Bold).Sprintf("[%s] ", h.Name)
			pterm.Println(prefix + "Connecting...")

			// 1. Initialize Transport
			var trans core.Transport
			var err error

			if h.Connection == "local" {
				trans = transport.NewLocalTransport()
			} else {
				// Determine port
				port := h.Port
				if port == 0 {
					port = 22
				}

				sshConfig := config.Host{
					Name:       h.Name,
					Address:    h.Address,
					User:       h.User,
					Port:       port,
					SSHKeyPath: h.KeyPath,
				}
				// Set timeout for connection
				ctx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
				defer cancel()

				trans, err = transport.NewSSHTransport(ctx, sshConfig)
				if err != nil {
					pterm.Error.Println(prefix + "Connection Failed: " + err.Error())
					errChan <- fmt.Errorf("[%s] connection failed: %w", h.Name, err)
					return
				}
			}
			defer trans.Close()

			// 2. Setup Context
			sysCtx := &core.SystemContext{
				Context:    context.Background(),
				Transport:  trans,
				FS:         trans.GetFileSystem(),
				DryRun:     f.DryRun,
				TargetUser: h.User,
			}

			// 3. Detect System
			system.Detect(sysCtx)
			pterm.Println(prefix + fmt.Sprintf("OS: %s / %s", sysCtx.Distro, sysCtx.Version))

			// 4. Create Engine
			engine := core.NewEngine(sysCtx, nil) // State updater per host TODO

			// 5. Execute Layers
			for i, layer := range layers {
				// Deep copy layer params for this host
				hostLayer := make([]core.ConfigItem, len(layer))
				for j, item := range layer {
					newItem := item
					// Copy Params map
					newItem.Params = make(map[string]interface{})
					for k, v := range item.Params {
						newItem.Params[k] = v
					}
					hostLayer[j] = newItem
				}

				// Merge host vars into context Vars
				if sysCtx.Vars == nil {
					sysCtx.Vars = make(map[string]string)
				}
				if h.Vars != nil {
					for k, v := range h.Vars {
						sysCtx.Vars[k] = v
					}
				}

				// Reuse Engine.RunParallel logic
				if err := engine.RunParallel(hostLayer, createFn); err != nil {
					pterm.Error.Printf("%s Layer %d Failed: %v\n", prefix, i+1, err)
					errChan <- fmt.Errorf("[%s] layer %d failed: %w", h.Name, i+1, err)
					return // Stop this host
				}
			}

			pterm.Success.Println(prefix + "Completed Successfully")

		}(host)
	}

	wg.Wait()
	close(errChan)

	// Collect errors
	errCount := 0
	for range errChan {
		errCount++
	}

	if errCount > 0 {
		return fmt.Errorf("fleet execution failed on %d hosts", errCount)
	}
	pterm.Success.Println("Fleet execution completed successfully.")
	return nil
}
